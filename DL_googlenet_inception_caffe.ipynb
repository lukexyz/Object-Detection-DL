{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Single Tensor shape: (1, 3, 224, 224)\n",
      "Multiple Tensors shape: (2, 3, 224, 224)\n",
      "\n",
      "Inception model info:\n",
      "gflops: 3.1904431360000003\n",
      "weights (mb): 27.994208 , blobs (mb): 45.92096\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open image and convert to tensor\n",
    "image = cv2.imread('data/Lena.png', cv2.IMREAD_COLOR)\n",
    "\n",
    "# Creating 4-dimensional blog from image\n",
    "\n",
    "tensor = cv2.dnn.blobFromImage(image, scalefactor=1.0, size=(224,224),\n",
    "                               mean=(104,117, 123), swapRB=False, crop=False)\n",
    "\"\"\"\n",
    "Docstring:\n",
    "    cv2.dnn.blobFromImage\n",
    "    blobFromImage(image, scalefactor, size, mean, swapRB, crop) \n",
    "    \n",
    "    1. The function resizes the image. If the crop flag is True, the input image is resized\n",
    "    while preserving the aspect ratio. One dimension (width or height) of the image\n",
    "    is set to a desirable value and the other is set equal or greater than the\n",
    "    corresponding value in the size argument. Then, the resulting image from the\n",
    "    center is cropped to the necessary size. If the crop flag is False, the function just\n",
    "    resizes to the target spatial size.\n",
    "\n",
    "    2. The function converts the values of the resized image to a floating-point type, if\n",
    "    necessary.\n",
    "\n",
    "    3. The function swaps the first and last channels if the corresponding argument is\n",
    "    True. This is necessary because OpenCV gives images in the BGR channel order\n",
    "    after loading, but some Deep Learning models may be trained for images with\n",
    "    the RGB channel order.\n",
    "\n",
    "    4. The function then subtracts the mean value from each pixel of the image. The\n",
    "    corresponding argument may be either a three-value tuple or just a one-value\n",
    "    tuple. If it is a three-value tuple, each value is subtracted from the corresponding\n",
    "    channel after the channels are swapped. If it's a single value, it is subtracted from\n",
    "    each channel.\n",
    "\n",
    "    5. Multiply the resulting image by the scale factor (2nd argument).\n",
    "\n",
    "    6. Convert the three-dimensional image to a four-dimensional tensor with an\n",
    "    NCHW order of dimensions.\n",
    "    \n",
    "    It's important to say that the preprocessing must be the same as it was while training the\n",
    "    model. Otherwise, the model may work poorly or even not work at all. If you've trained the\n",
    "    model yourself, you know all the parameters. But if you've found the model on the internet,\n",
    "    you need to examine the description of the model or training scripts to get the necessary\n",
    "    information.\n",
    "\"\"\"\n",
    "\n",
    "# Tensor from multiple images:\n",
    "tensors = cv2.dnn.blobFromImages([image, image], scalefactor=1.0, size=(224,224),\n",
    "                               mean=(104,117, 123), swapRB=False, crop=False)\n",
    "\n",
    "print(\"   Single Tensor shape:\", tensor.shape)\n",
    "print(\"Multiple Tensors shape:\", tensors.shape)\n",
    "\n",
    "# ==============================================================\n",
    "# Load GoogleNet pre-trained Inception NN (ImageNet winner 2014)\n",
    "# http://deeplearning.net/tag/googlenet/\n",
    "# ==============================================================\n",
    "model = cv2.dnn.readNetFromCaffe('data/bvlc_googlenet.prototxt', \n",
    "                               'data/bvlc_googlenet.caffemodel')\n",
    "\n",
    "# ==============================================================\n",
    "# Report model Info\n",
    "# ==============================================================\n",
    "print('\\nInception model info:')\n",
    "print('gflops:', model.getFLOPS((1,3,224,224))*1e-9)\n",
    "w,b = model.getMemoryConsumption((1,3,224,224))\n",
    "print('weights (mb):', w*1e-6, ', blobs (mb):', b*1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference (ms): 126.380455\n",
      "\n",
      "LAYER                          TIME (ms)\n",
      "0. conv1/7x7_s2                   13.27\n",
      "1. conv1/relu_7x7                 0.00\n",
      "2. pool1/3x3_s2                   1.51\n",
      "3. pool1/norm1                    2.86\n",
      "4. conv2/3x3_reduce               1.25\n",
      "5. conv2/relu_3x3_reduce          0.00\n",
      "6. conv2/3x3                      18.92\n",
      "...\n",
      "135. inception_5b/pool_proj         0.46\n",
      "136. inception_5b/relu_pool_proj    0.00\n",
      "137. inception_5b/output            0.20\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# Perform a single forward-pass inference and calculate timings\n",
    "# ==============================================================\n",
    "\n",
    "model.setInput(tensors)\n",
    "prob = model.forward()\n",
    "\n",
    "# Timing machinery\n",
    "total, timings = model.getPerfProfile()\n",
    "tick2ms = 1e3/cv2.getTickFrequency()\n",
    "print('Inference (ms): {:2f}\\n'.format(total*tick2ms))\n",
    "\n",
    "layer_names = model.getLayerNames()\n",
    "print('{: <30} {}'.format('LAYER', 'TIME (ms)'))\n",
    "for (i,t) in enumerate(timings[0:7]):\n",
    "    print('{}. {: <30} {:.2f}'.format(i, layer_names[i], t[0]*tick2ms))\n",
    "print('...')\n",
    "for (i,t) in enumerate(timings[135:138]):\n",
    "    i += 135\n",
    "    print('{}. {: <30} {:.2f}'.format(i, layer_names[i], t[0]*tick2ms))\n",
    "# =============================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# GoogleNet Image Classification\n",
    "# ==============================================================\n",
    "\n",
    "def classify(video_src, net, in_layer, out_layer,\n",
    "             mean_val, category_names, swap_channels=False):\n",
    "    \"\"\"\n",
    "    Classification function for GoogleNet neural network.\n",
    "\n",
    "        1. Gets frames from a videos,\n",
    "        2. Transforms them into tensors,\n",
    "        3. Forward feed into neural network,\n",
    "        4. Selects the highest probability out of five categories\n",
    "        \n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_src)\n",
    "    t = 0\n",
    "    while True:\n",
    "        # --------------------- Get Video Frame and Status ------------------ #\n",
    "        status_cap, frame = cap.read()\n",
    "        if not status_cap:\n",
    "            break\n",
    "        frame = cv2.resize(frame, None, fx=0.25, fy=0.25)\n",
    "        \n",
    "        if isinstance(mean_val, np.ndarray):\n",
    "            tensor = cv2.dnn.blobFromImage(frame, 1.0, (224, 224), 1.0, False)\n",
    "            tensor -= mean_val\n",
    "        else:\n",
    "            tensor = cv2.dnn.blobFromImage(frame, 1.0, (224, 224), mean_val, swap_channels)\n",
    "        \n",
    "        # --------------------- Forward Propagation of NN ------------------ #\n",
    "        net.setInput(tensor, in_layer)\n",
    "        prob = net.forward(out_layer)\n",
    "        prob = prob.flatten()\n",
    "        \n",
    "        # ---------------------- Add Prediction to Frame ------------------- #\n",
    "        r = 1\n",
    "        for i in np.argsort(prob)[-5:]:\n",
    "            txt = ' \"%s\"; probability: %.2f' % (category_names[i], prob[i])\n",
    "            cv2.putText(img=frame,\n",
    "                        text=txt,\n",
    "                        org=(0, frame.shape[0] - r*20), \n",
    "                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=0.7,\n",
    "                        color=(0, 255, 0),\n",
    "                        thickness=2);\n",
    "            r += 1\n",
    "            \n",
    "        # --------------------------- Flow Control ------------------------ #\n",
    "        cv2.imshow('classification', frame)\n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "        \n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageNet classes: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tench, Tinca tinca',\n",
       " 'goldfish, Carassius auratus',\n",
       " 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
       " 'tiger shark, Galeocerdo cuvieri',\n",
       " 'hammerhead, hammerhead shark',\n",
       " 'electric ray, crampfish, numbfish, torpedo',\n",
       " 'stingray',\n",
       " 'cock',\n",
       " 'hen',\n",
       " 'ostrich, Struthio camelus']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# Show ImageNet categories\n",
    "# ==============================================================\n",
    "\n",
    "with open('data/synset_words.txt') as f:\n",
    "    class_names = [' '.join(l.split(' ')[1: ]).rstrip() for l in f.readlines()]\n",
    "print(\"ImageNet classes:\", class_names.__len__())\n",
    "class_names[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Classify video with Inception NN\n",
    "# ==============================================================\n",
    "\n",
    "googlenet_caffe = cv2.dnn.readNetFromCaffe('data/bvlc_googlenet.prototxt', \n",
    "                                           'data/bvlc_googlenet.caffemodel')\n",
    "\n",
    "# video_path = \"data/Traffic.mp4\"\n",
    "video_path = \"data/shuttle.mp4\"\n",
    "\n",
    "classify(video_path, googlenet_caffe, 'data', 'prob', (104, 117, 123), class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_caffe = cv2.dnn.readNetFromCaffe('../data/resnet_50.prototxt', \n",
    "                                           '../data/resnet_50.caffemodel')\n",
    "mean = np.load('../data/resnet_50_mean.npy')\n",
    "\n",
    "classify('../data/shuttle.mp4', resnet_caffe, 'data', 'prob', mean, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
